{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44f35f9-6fcf-4a16-8348-d0fd1c959858",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "There's a lot of ways to tackle this problem, but I narrowed it down to a few ways that might work that we're familiar with just from the topics from the class already.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "We have two inputs: IMU data, and images. Each comes in streams of different frequencies. Assuming the IMU data comes in at a frequency of X Hz, and the image data comes in at a lower frequency of Y Hz, we need to run our model at Y Hz using a single image X/Y number of samples from the IMU associated with that image.\n",
    "\n",
    "We have an anomaly detection problem, as we need to make sense of our inputs and determine if something bad is happening. Thus, our model needs to learn what is considered normal SLAM and flight behavior for our drone. Since our system is evolving over time, we have two options. \n",
    "\n",
    "1. Make the model output some binary label that corresponds to SLAM failure or nominal operations. Alternatively, this can also be converted to a confidence using softmax, if we want. Then the model will predict if there is a SLAM failure within a given window, which we can compare to the GT trajectory of the drone. If the two diverge by a certain amount, label that point as a SLAM failure. Then we can run the model on the test set and we will manually label each window of the test as failure or nominal, based on whatever SLAM-VIO algorithm both us and the drone will use. This model will use binary cross entropy as the loss function, which will maximize the classification likelihood.\n",
    "2. Make the model predict a path with initial condition being its current estimated point. Its output would be the path divergence over time, say the average within the next cycle, and we could conceive of a metric that determines if this is a SLAM failure or not. But the model will be training on the actual raw position data, instead of failure labels. This model will use L2 loss per physical direction (x,y,z), which will penalize large path estimation residuals. This will maximize the likelihood the model predicts the true path at each timestep.\n",
    "    1. A prediction-based RNN can learn to forecast future pose readings. If the actual SLAM readings deviate heavily from the predicted readings, we can have the model flag that as a SLAM failure, as the SLAM algorithm is not functioning as expected which is a sign of failure. Models following this paradigm are outlined in [Darba et al. (2023)](https://arxiv.org/html/2211.05244v3#:~:text=learning%20architectures%20,based%20models%20are%20more%20effective).\n",
    "\n",
    "I prefer case 2 as it is more grounded in its physical interpretations and can give us more things to plot and show for our presentation. Something to note is that anomalies are not labeled in our training data, so it is hard to make the model learn anomalies. Rather, what anomaly detection entails usually is learning **normal** behavior, and detecting abnormal behavior during testing. \n",
    "\n",
    "## RNN-based SLAM Anomaly Detector\n",
    "\n",
    "If we go with model 2, we can make a RNN scheme that follows this data pipeline:\n",
    "\n",
    "1. IMU branch (200 Hz): Processes raw IMU packets with a small RNN to produce one embedding per camera frame.\n",
    "\n",
    "1. Vision branch (20 Hz): Encodes each image with a CNN backbone + optional per‐frame RNN to capture appearance dynamics.\n",
    "\n",
    "1. Fusion RNN (20 Hz): At each camera frame, takes the concatenated IMU & vision embeddings and updates a shared hidden state.\n",
    "\n",
    "1. Prediction head: From the fusion RNN’s hidden state, outputs a prediction of the next-cycle pose sequence (Δx,Δy,Δz over the next N frames) or directly the expected average drift.\n",
    "\n",
    "1. Anomaly flag: During inference, compute the L₂ error between true SLAM output and your model’s prediction. If error > threshold → SLAM failure.\n",
    "\n",
    "\n",
    "### IMU Branch\n",
    "\n",
    "For this branch, the IMU samples are much faster than the camera, so collect them into a vector between the previous and current camera frame. This can be done with either a small RNN or a 1D CNN.\n",
    "\n",
    "`[6×10] → Conv1D(filters=32,kernel=3,stride=1) → ReLU → Conv1D(64,3,1) → ReLU\n",
    "       → GlobalAvgPool → Dense(D) → IMU_embed_t`\n",
    "\n",
    "`for i in 1…10: h_i = GRU_cell(u_{t,i}, h_{i-1})\n",
    "IMU_embed_t = Dense(h_{10})`\n",
    "\n",
    "Input: 10 IMU samples (200 vs 20 Hz sampling), {u<sub>1</sub>, ..., u<sub>10</sub>}, where u is a 6 dim vector of IMU data.\n",
    "\n",
    "Output: One learned embedding of IMU data per camera frame of D dimension. \n",
    "\n",
    "### Vision Branch\n",
    "\n",
    "This branch is just a regular vision branch but can also include a RNN to capture temporal changes. That would look like:\n",
    "\n",
    "1. CNN -> GAP (global average pooling) -> linear projection\n",
    "    1. Image(t) ---> e<sub>t</sub> in R<sup>d</sup>\n",
    "1. RNN update (here I use LSTM with hidden state h and cell c)\n",
    "    1. (h<sub>t-1</sub>, c<sub>t-1</sub>), e<sub>t</sub> -> (h<sub>t</sub>, c<sub>t</sub>),\n",
    "1. Hidden state -> vision embedding\n",
    "    1. h<sub>t</sub> ---> Vision(t) in R<sup>d</sup>\n",
    "\n",
    "Since this is the big CNN for image processing, it will take up the bulk of the computation. We should consider using existing models like MobileNet which are optimized to be more lightweight, especially since our project is more robotics oriented, and the extra weight savings will contribute a lot to real world applicability.\n",
    "\n",
    "### Fusion RNN\n",
    "\n",
    "This will fuse the two branches using a RNN to capture temporal relationships between IMU data and camera data. \n",
    "\n",
    "Concatenate vision and IMU embeeddings into a 2*D vector and pass into another RNN: `fus_h_t, fus_c_t = LSTM_cell(x_t, (fus_h_{t-1}, fus_c_{t-1}))`\n",
    "\n",
    "### Prediction Head\n",
    "\n",
    "The final part is predicting whatever we need to predict. We can do this one of two ways. \n",
    "\n",
    "First, we can get the expected future frames K for some small integer K, calculating the change in displacement and comparing that to actual change in displacement. At inference, we can find the mean and standard deviation of loss and if it is above some threshold, declare a failure.\n",
    "\n",
    "Alternatively, we can direcly predict L2 drift in the next cycle. If that is above some threshold we set, declare failure. The training should minimize this drift if we set the drift to be the loss function itself, and whenever the model encounters high loss, it will see whatever is happening is very different from what it's used to, which is good flying data, and declare failure.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    IMU[IMU 200Hz] --> RNN1[RNN or 1D CNN]\n",
    "    RNN1 --> I[IMU Feature Iₜ]\n",
    "\n",
    "    CAM[Camera 20Hz] --> ResNet[ResNet CNN]\n",
    "    ResNet --> SAP\n",
    "    SAP --> Linear\n",
    "    Linear --> V[Cam Feature Vₜ]\n",
    "\n",
    "    I --> RNN2\n",
    "    V --> RNN2\n",
    "\n",
    "    RNN2 --> Head[MLP or RNN]\n",
    "    Head --> Drift[Drift Lₐ]\n",
    "    Head --> Displacement[Next k Δ displacement]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c08dd4-4fdb-4002-9e60-6c7d7503fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nn.Module using lightning (this is old pytorch lightning, new one is just lightning as L)\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "\n",
    "class DriftPredictor(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = MyCustomNet()  # Define as nn.Module\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = self.criterion(preds, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bab943-f516-47fd-830a-c606330993eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example lightning code for RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "# ---- Dummy Dataset ----\n",
    "class DummyIMUDataset(Dataset):\n",
    "    def __init__(self, seq_len=10, num_samples=1000):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imu_seq = torch.randn(self.seq_len, 6)  # e.g. 3-axis accel + 3-axis gyro\n",
    "        delta_p = imu_seq.sum(dim=0)[-3:] * 0.1  # fake \"position delta\"\n",
    "        return imu_seq, delta_p\n",
    "\n",
    "\n",
    "# ---- Lightning Module ----\n",
    "class DriftPredictorRNN(pl.LightningModule):\n",
    "    def __init__(self, input_dim=6, hidden_dim=64, output_dim=3, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.rnn(x)  # hn: (num_layers, batch, hidden_dim)\n",
    "        return self.fc(hn[-1])  # Take final hidden state\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ---- Data ----\n",
    "train_ds = DummyIMUDataset()\n",
    "val_ds = DummyIMUDataset()\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "\n",
    "# ---- Train ----\n",
    "model = DriftPredictorRNN()\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"cpu\")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc50a2-a09b-40d2-a7dd-609244551f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
